{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "working-fraction",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: MLflow support for Python 3.6 is deprecated and will be dropped in an upcoming release. At that point, existing Python 3.6 workflows that use MLflow will continue to work without modification, but Python 3.6 users will no longer get access to the latest MLflow features and bugfixes. We recommend that you upgrade to Python 3.7 or newer.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape_score\n",
    "\n",
    "from corn_utils import (\n",
    "    load_all_csvs,\n",
    "    load_all_jsons,\n",
    "    join_csvs_and_filter_by_year,\n",
    "    unnorm_a_column,\n",
    "    get_scores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ideal-heather",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lst, y_train_lst, X_test_lst, y_test_lst = [],[],[],[]\n",
    "\n",
    "for state in [\n",
    "    'Minnesota',\n",
    "#     'South_Dakota',\n",
    "#     'North_Dakota',\n",
    "    'Nebraska',\n",
    "    'Iowa'\n",
    "]:\n",
    "    df_meteo = load_all_jsons(state=state)\n",
    "    df_target = join_csvs_and_filter_by_year(load_all_csvs(state=state.upper()))\n",
    "    \n",
    "    df = pd.concat([\n",
    "        df_meteo[['NLST', 'Nndvi', 'Npr', 'Ntemp','Nvpd']].groupby(level=0).mean(),\n",
    "        df_target[[\n",
    "#             'Nharvest',\n",
    "            'Nprod',\n",
    "        ]].groupby(level=0).sum(),\n",
    "        df_target[[\n",
    "            'Nyield'\n",
    "        ]].groupby(level=0).mean(),\n",
    "    ], axis=1)\n",
    "\n",
    "    XY = df.to_numpy()\n",
    "    X_train_el, y_train_el = XY[:-4,:5], XY[:-4,5:]\n",
    "    X_test_el, y_test_el = XY[:-4:-1,:5][::-1,:], XY[:-4:-1,5:][::-1,:] # 2020,2021,2022\n",
    "    \n",
    "    X_train_lst.append(X_train_el)\n",
    "    y_train_lst.append(y_train_el)\n",
    "    X_test_lst.append(X_test_el)\n",
    "    y_test_lst.append(y_test_el)\n",
    "    \n",
    "X_train = np.concatenate(X_train_lst)\n",
    "y_train = np.concatenate(y_train_lst)\n",
    "X_test = np.concatenate(X_test_lst)\n",
    "y_test = np.concatenate(y_test_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tropical-wagner",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def full_training_eval_loop(\n",
    "    hidden_layer_sizes=(5)\n",
    "):\n",
    "\n",
    "    with mlflow.start_run(\n",
    "        run_name=str(hidden_layer_sizes),\n",
    "        tags={\"hidden_layer_sizes\": str(hidden_layer_sizes)},\n",
    "    ):\n",
    "        mlflow.log_param(\"hidden_layer_sizes\", str(hidden_layer_sizes))\n",
    "        \n",
    "        mlflow.log_param(\n",
    "            \"randomstatelst\",\n",
    "            '4691,6231,7298,2554,7856,5509,9803,7778,4314,9261,4436,4883'\n",
    "        )\n",
    "\n",
    "        dd, ddtrain, losscurve = [], [], []\n",
    "\n",
    "        for random_state in [\n",
    "            4691,6231,7298,2554,\n",
    "            7856,5509,9803,7778,\n",
    "            4314,9261,4436,4883\n",
    "        ]:\n",
    "\n",
    "            regr = MLPRegressor(\n",
    "            #     batch_size=3,\n",
    "                random_state=random_state,\n",
    "                learning_rate_init=0.01,\n",
    "                max_iter=10000,\n",
    "                hidden_layer_sizes=hidden_layer_sizes,\n",
    "                validation_fraction=0.1,\n",
    "                n_iter_no_change=10000,\n",
    "            ).fit(X_train, y_train)\n",
    "            \n",
    "            losscurve.append(regr.loss_curve_)\n",
    "\n",
    "            y_test_nonull = y_test[~np.isnan(y_test).any(axis=1), :]\n",
    "            y_pred_nonull = regr.predict(X_test)[~np.isnan(y_test).any(axis=1), :]\n",
    "\n",
    "            d = get_scores(\n",
    "                y_test_nonull,\n",
    "                y_pred_nonull,\n",
    "                df_target\n",
    "            )\n",
    "\n",
    "            dd.append(d)\n",
    "\n",
    "            dtrain = get_scores(\n",
    "                y_train,\n",
    "                regr.predict(X_train),\n",
    "                df_target\n",
    "            )\n",
    "\n",
    "            ddtrain.append(dtrain)\n",
    "\n",
    "        avg_d_test = {'r2': [\n",
    "            np.mean([d['r2'][0] for d in dd]),\n",
    "            np.mean([d['r2'][1] for d in dd])\n",
    "        ], 'mape': [\n",
    "            np.mean([d['mape'][0] for d in dd]),\n",
    "            np.mean([d['mape'][1] for d in dd])\n",
    "        ]}\n",
    "\n",
    "        avg_d_train = {'r2': [\n",
    "            np.mean([d['r2'][0] for d in ddtrain]),\n",
    "            np.mean([d['r2'][1] for d in ddtrain])\n",
    "        ], 'mape': [\n",
    "            np.mean([d['mape'][0] for d in ddtrain]),\n",
    "            np.mean([d['mape'][1] for d in ddtrain])\n",
    "        ]}\n",
    "        \n",
    "        median_loss_curve = np.asarray(losscurve).mean(axis=0)\n",
    "        \n",
    "        for step,val in enumerate(median_loss_curve):\n",
    "            mlflow.log_metric(key='loss', value=val, step=step)\n",
    "            \n",
    "        mlflow.log_metric('r2trainProd', avg_d_train['r2'][0])\n",
    "        mlflow.log_metric('r2trainYield', avg_d_train['r2'][1])\n",
    "        mlflow.log_metric('r2testProd', avg_d_test['r2'][0])\n",
    "        mlflow.log_metric('r2testYield', avg_d_test['r2'][1])\n",
    "        \n",
    "        mlflow.log_metric('MAPEtrainProd', avg_d_train['mape'][0])\n",
    "        mlflow.log_metric('MAPEtrainYield', avg_d_train['mape'][1])\n",
    "        mlflow.log_metric('MAPEtestProd', avg_d_test['mape'][0])\n",
    "        mlflow.log_metric('MAPEtestYield', avg_d_test['mape'][1])\n",
    "\n",
    "        return avg_d_test, avg_d_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "honest-dominant",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/11/08 21:53:51 WARNING mlflow.tracking.context.git_context: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh()\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial warning can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|none|n|0: for no warning or exception\n",
      "    - warn|w|warning|1: for a printed warning\n",
      "    - error|e|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'r2': [0.9187248448149578, 0.8032725538881978],\n",
       "  'mape': [3.688003650101692, 6.035560390158171]},\n",
       " {'r2': [0.9638804155515123, 0.9657376872982001],\n",
       "  'mape': [2.261633346052282, 2.9633168199345463]})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_training_eval_loop(hidden_layer_sizes=(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "verbal-journalist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'r2': [0.8867261656935318, 0.7833908935448309],\n",
       "  'mape': [4.3792956593209, 6.04984021504762]},\n",
       " {'r2': [0.9841313807687465, 0.980728844356478],\n",
       "  'mape': [1.5216037141078285, 2.4836567258030224]})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_training_eval_loop(hidden_layer_sizes=(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "parental-google",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'r2': [0.9084100737640414, 0.7858935830376033],\n",
       "  'mape': [4.341956442113097, 6.638426079509535]},\n",
       " {'r2': [0.9768902828429042, 0.968197584214134],\n",
       "  'mape': [1.6403949568362932, 2.3696076760873677]})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_training_eval_loop(hidden_layer_sizes=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "flexible-machinery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'r2': [0.8929061257702265, 0.8201836097434877],\n",
       "  'mape': [4.693483821355433, 5.562554719693572]},\n",
       " {'r2': [0.958449175515267, 0.9638004020487957],\n",
       "  'mape': [2.2816156756107957, 3.1342818269191657]})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_training_eval_loop(hidden_layer_sizes=(5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cardiovascular-browse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'r2': [0.8831590762952626, 0.6522943992344461],\n",
       "  'mape': [4.916295215886383, 8.01487337834953]},\n",
       " {'r2': [0.9958424223528003, 0.9802106720653906],\n",
       "  'mape': [0.6055672473114667, 1.9924026510876356]})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_training_eval_loop(hidden_layer_sizes=(5,5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cardiovascular-toddler",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'r2': [0.9275366830996891, 0.8790743156811235],\n",
       "  'mape': [3.4488343685961538, 4.498669933157642]},\n",
       " {'r2': [0.9843236705054877, 0.9805189408705138],\n",
       "  'mape': [1.0048698397445814, 1.9613099010753634]})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_training_eval_loop(hidden_layer_sizes=(5,5,5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-onion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-portugal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
